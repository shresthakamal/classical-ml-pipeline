{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffd8b738-03d4-4eef-ad4a-1a4234150a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kamal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kamal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kamal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# 1. lower case\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 2. remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 3. remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# 4. remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 5. remove short words\n",
    "def remove_short_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if len(w) > 2]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# 6. lemmatize\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "# 8. remove non-ascii characters\n",
    "def remove_non_ascii(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "# 9. remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "# 16. remove extra non-breaking spaces\n",
    "def remove_extra_non_breaking_spaces(text):\n",
    "    return re.sub('\\xa0+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1997cc28-7f69-4752-9cb4-b3326fe75155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_title(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = lower_case(text)\n",
    "    text = lemmatize(text)\n",
    "    text = remove_extra_non_breaking_spaces(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def process_keywords(text):\n",
    "    # split the keywords based on \";\"\n",
    "    tokens = text.split(\";\")\n",
    "    tokens = [remove_stop_words(t) for t in tokens]\n",
    "    tokens = [remove_non_ascii(t) for t in tokens ]\n",
    "\n",
    "    \n",
    "    tokens = [t.split(\"/\") for t in tokens]\n",
    "    tokens = list(itertools.chain.from_iterable(tokens)) \n",
    "    \n",
    "    tokens = [remove_extra_non_breaking_spaces(t) for t in tokens]\n",
    "    tokens = [lemmatize(t) for t in tokens ]\n",
    "\n",
    "    tokens = [lower_case(t) for t in tokens]\n",
    "\n",
    "    return \" \".join(tokens) \n",
    "\n",
    "def process_target(label):\n",
    "    label_map={\n",
    "            \"Relevant\":1,\n",
    "            \"Not relevant\":0\n",
    "    }\n",
    "    return label_map[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41b7ceae-1090-4009-8442-d81479c1a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        \n",
    "    def vectorize(self, sentences):\n",
    "        vectorizer = TfidfVectorizer(**self.params)\n",
    "        vectors = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        return vectors, vectorizer\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.vectorizer = TFIDFVectorizer(**params[\"vectorizer\"])\n",
    "        \n",
    "        self.train_transform = self.params[\"train_transform\"]\n",
    "        self.test_transform = self.params[\"test_transform\"]\n",
    "        \n",
    "    def serializer(self, object=None, path=None, mode=\"save\"):\n",
    "        if mode==\"save\":\n",
    "            with open(path, 'wb') as handle:\n",
    "                pickle.dump(object, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            return True\n",
    "            \n",
    "        if mode==\"load\":\n",
    "            with open(path, 'rb') as handle:\n",
    "                object = pickle.load(handle)\n",
    "            return object\n",
    "            \n",
    "    def __dimension_check(self, ndim):\n",
    "        if ndim < self.params[\"vectorizer\"][\"max_features\"]:\n",
    "            raise Exception (f\"\"\"Maximum features is more than number of TFIDF tokens, {ndim} < {self.params[\"vectorizer\"][\"max_features\"]}\"\"\")\n",
    "    \n",
    "    def process(self, df, mode):      \n",
    "        feature_vectors = []\n",
    "\n",
    "        if mode==\"train\":\n",
    "\n",
    "            feature_vectorizers = {}\n",
    "            \n",
    "            for column, transform in self.train_transform.items():\n",
    "                df[column] = df[column].apply(transform)\n",
    "                \n",
    "                if column!=\"target\":\n",
    "                    vecs, vectorizer = self.vectorizer.vectorize(df[column].values)\n",
    "                    self.__dimension_check(vecs.toarray().shape[1])                    \n",
    "                    \n",
    "                    feature_vectors.append(vecs.toarray())\n",
    "                    feature_vectorizers[column] = vectorizer\n",
    "                else:\n",
    "                    y = df[column].values\n",
    "\n",
    "            if self.serializer(feature_vectorizers, self.params[\"vectorizer_checkpoint\"], \"save\"):\n",
    "                print(\"Vectorizers successfully saved\")\n",
    "                    \n",
    "        elif mode==\"test\":\n",
    "            print(\"Loading saved vectorizers\")\n",
    "            \n",
    "\n",
    "            if not os.path.exists(self.params[\"vectorizer_checkpoint\"]):\n",
    "                raise Exception(\"Vectorizers missing for test\")\n",
    "\n",
    "            feature_vectorizers = self.serializer(path = self.params[\"vectorizer_checkpoint\"], mode=\"load\")\n",
    "                \n",
    "            for column, transform in self.test_transform.items():\n",
    "                df[column] = df[column].apply(transform)\n",
    "\n",
    "                if column!=\"target\":\n",
    "                    vecs = feature_vectorizers[column].transform(df[column].values)\n",
    "                    self.__dimension_check(vecs.toarray().shape[1])\n",
    "                    \n",
    "                    feature_vectors.append(vecs.toarray())\n",
    "                else:\n",
    "                    y = df[column].values\n",
    "        else:\n",
    "            raise Exception(\"Preprocessing mode is not identified\")\n",
    "                    \n",
    "        X = np.stack(feature_vectors, axis=1)\n",
    "        X = np.reshape(X, (X.shape[0], -1))\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b1a23abf-e265-4c49-b72b-5931487e1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"vectorizer\":{\n",
    "        \"max_features\":3475, \n",
    "        \"ngram_range\":(1,2)\n",
    "    },\n",
    "    \"train_transform\":{\n",
    "        \"title\":process_title,\n",
    "        \"keywords\":process_keywords,\n",
    "        \"target\":process_target\n",
    "    },\n",
    "    \"test_transform\":{\n",
    "        \"title\":process_title,\n",
    "        \"keywords\":process_keywords,\n",
    "        \"target\":process_target    \n",
    "    },\n",
    "    \"vectorizer_checkpoint\": \"vectorizers.pkl\"\n",
    "}\n",
    "\n",
    "processor = PreProcessor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "791d162a-a510-466a-96a7-7046a2e66d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../data/raw/20230821_full_data.xlsx\")\n",
    "\n",
    "test = pd.read_excel(\"../data/raw/test_ids.xlsx\")\n",
    "test_ids = list(test[\"id\"].values)\n",
    "\n",
    "train = data[~data[\"id\"].isin(test_ids)]\n",
    "test = data[data[\"id\"].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1c61471-1a10-4fab-a2e3-85ba33870376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizers successfully saved\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = processor.process(train, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7707ab7a-042b-4e48-b13b-a7c9f20feb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved vectorizers\n"
     ]
    }
   ],
   "source": [
    "test_X, test_y = processor.process(test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e753ec48-323d-48ce-9d7a-b6818f8890dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1595, 6950), (1595,), (786, 6950), (786,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd1e12d8-8e03-4aff-84a7-e58a8ad7d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ebeb32d-88b2-4871-8aff-d6be12278107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9028213166144201"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def model_accuracy(predictions, actual):\n",
    "    return accuracy_score(predictions, actual)\n",
    "\n",
    "clf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f78728de-0d4f-4996-9728-50633255491f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7239185750636132"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict(test_X)\n",
    "\n",
    "accuracy = model_accuracy(predictions, test_y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53f9b0fb-539b-4212-b85b-0235b4c24ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test_X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60538e48-2ba9-40b6-ae4b-c015da79146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0, criterion=\"entropy\")\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6292485f-3a6f-4f6f-876b-65db618d5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2358a447-0ba7-40e6-aeee-a37a426b72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7442748091603053"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict(test_X)\n",
    "\n",
    "accuracy = model_accuracy(predictions, test_y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc96aa-52a8-4059-b292-cf6c89f93e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cedf6-fe4c-4da8-8e51-9aeb2bd5c4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
